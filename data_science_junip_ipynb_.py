# -*- coding: utf-8 -*-
"""Копия блокнота "Data Science Junip.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18jf38rIenrTK2-i_-M7ha8hI5vhyn6jo
"""

# Commented out IPython magic to ensure Python compatibility.
#Pip Installs
!pip install pandas
!pip install nlpaug
!pip install sklearn
!pip install spacy
!pip install sentencepiece
!pip install torch
!pip install transformers
!pip install matplotlib
!pip install numpy
!pip install deep-translator
!pip install sacremoses

#Import math,dfs,etc...
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
# %matplotlib inline
import math
from collections import defaultdict
import tqdm

#Import Classification Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn import tree

#Import LE, Training, Statistics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix
import joblib


#Import Tokenization / Features
import spacy
from spacy import displacy
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import string
import sentencepiece
import torch
import torch.nn as nn
from torch.optim import Adam
from transformers import CamembertTokenizer, CamembertModel
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from deep_translator import GoogleTranslator
device = torch.device('mps')

"""# ***Data Preparation***

"""

train = pd.read_csv('https://raw.githubusercontent.com/GroupTissot/DSMLFinalProject/main/unchanged_trainingdata.csv').drop(columns=['id'])
test = pd.read_csv('https://raw.githubusercontent.com/GroupTissot/DSMLFinalProject/main/unlabelled_test_data.csv').drop(columns=['id'])
display(train)
display(test)

train = train.drop_duplicates()
train = train.sample(frac=1, random_state=42).reset_index(drop=True)
train

"""##Back Translation Script"""

#This Script was essentially ran on it's own to google translate text to english
#You'd then have to take the translated text, swap the intermediate and target languages and translate again to "back" translate.
#This can take up to 2 hours PER translation. It's not worth doing, but the output we created can be found on our github.
'''
# Global counter for translations
translation_count = 0

def back_translate(text, translator, intermediate_language='en'):
    global translation_count

    # Translate to an intermediate language
    translated = translator.translate(text, target=intermediate_language)

    # Increment and print the counter after the first translation
    translation_count += 1
    print(f"Translation Count: {translation_count}")

    # Translate back to French
    back_translated = translator.translate(translated, target='fr')

    # Increment and print the counter after the second translation
    translation_count += 1
    print(f"Translation Count: {translation_count}")

    return back_translated

def main():
    # Load the dataset
    url = "/content/back_translated_dataset.csv"
    df = pd.read_csv(url)

    # Initialize the Google Translator
    translator = GoogleTranslator(source='auto', target='fr')

    # Perform back translation on the 'sentence' column
    df['sentence'] = df['back_translated_sentence'].apply(lambda x: back_translate(x, translator))

    # Create a new DataFrame with back-translated sentences and their difficulty
    new_df = df[['sentence', 'difficulty']]

    # Save the new DataFrame to a CSV file
    new_df.to_csv('back_translated_sentencedataset.csv', index=False)

if __name__ == "__main__":
    main()


'''

"""# ***Tokenization & Text features (BERT)***"""

X = train['sentence']
y = train['difficulty']

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')

tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base', num_labels=6).to(device)


def bert_feature(data, **kwargs):
    # Tokenize and encode input texts
    input_ids = [tokenizer.encode(text, add_special_tokens=True, **kwargs) for text in data]

    # Extract BERT features for each input ID
    features = []
    with torch.no_grad():
        for input_id in tqdm.tqdm(input_ids):
            # Convert input ID to tensor
            input_tensor = torch.tensor(input_id).unsqueeze(0).to(device)

            # Extract BERT features for this input ID
            input_embeds = model.embeddings(input_tensor)
            feature = model(inputs_embeds=input_embeds)[0][:, 0, :].cpu().numpy()

            # Add feature to list of all features
            features.append(feature)

    # Concatenate features from all inputs
    feature_data = np.concatenate(features, axis=0)

    torch.cuda.empty_cache()

    return feature_data

X = bert_feature(X)

"""# ***Classification Models***

## Defining our Evaluation & Encoder
"""

def evaluation(model, Xtest, ytest):
    ypred = model.predict(Xtest)
    precision = precision_score(ytest, ypred, average='macro')
    recall = recall_score(ytest, ypred, average='macro')
    f1 = f1_score(ytest, ypred, average='macro')
    accuracy = accuracy_score(ytest, ypred)
    eval = [accuracy, precision, recall, f1]
    return eval

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.1, random_state=42)

# Define the encoder
le = LabelEncoder()
# Fit the encoder
le.fit(ytrain)

ytrain = le.transform(ytrain)
ytest = le.transform(ytest)

model_comparison = pd.DataFrame(index=['Accuracy', 'Precision', 'Recall', 'F1'])

"""##Logistical Regression Model"""

logre = LogisticRegression()
logre.fit(Xtrain,ytrain)

model_comparison['Logistic Regression'] = evaluation(logre, Xtest, ytest)
model_comparison

"""##kNN Model"""

knn = KNeighborsClassifier(algorithm="kd_tree")
knn.fit(Xtrain,ytrain)

model_comparison['KNN'] = evaluation(knn, Xtest, ytest)
model_comparison

"""## Decision Tree Model"""

trees = DecisionTreeClassifier()
trees.fit(Xtrain,ytrain)

model_comparison['Decision Tree'] = evaluation(trees, Xtest, ytest)
model_comparison

"""##Random Forest Model"""

rfc = RandomForestClassifier(random_state=42)
rfc.fit(Xtrain,ytrain)

model_comparison['Random Forest'] = evaluation(rfc, Xtest, ytest)
model_comparison

"""## Random Forest Model Refined"""

rfcrefined = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=300, random_state=42)
rfcrefined.fit(Xtrain,ytrain)

model_comparison['Random Forest Refined'] = evaluation(rfcrefined, Xtest, ytest)
model_comparison

"""## Extra Trees Model"""

etc = ExtraTreesClassifier(random_state=42)
etc.fit(Xtrain,ytrain)

model_comparison['Extra Trees'] = evaluation(etc, Xtest, ytest)
model_comparison

"""## Extra Trees Model Refined"""

etcrefined = ExtraTreesClassifier(min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=42)
etcrefined.fit(Xtrain,ytrain)

model_comparison['Extra Trees Refined'] = evaluation(etcrefined, Xtest, ytest)
model_comparison

"""## LightGBM Model"""

lgbm = LGBMClassifier()
lgbm.fit(Xtrain,ytrain)

model_comparison['LightGBM'] = evaluation(lgbm, Xtest, ytest)
model_comparison

"""## LightGBM Model Refined"""

lgbmrefined = LGBMClassifier(learning_rate=0.2, max_depth=7)
lgbmrefined.fit(Xtrain,ytrain)

model_comparison['LightGBM Refined'] = evaluation(lgbmrefined, Xtest, ytest)
model_comparison

"""## XGBoost Model

"""

xgb = XGBClassifier()
xgb.fit(Xtrain,ytrain)

model_comparison['XGBoost'] = evaluation(xgb, Xtest, ytest)
model_comparison

"""## XGBoost Model Refined"""

xgbrefined = XGBClassifier(learning_rate=0.2, max_depth=9)
xgbrefined.fit(Xtrain,ytrain)

model_comparison['XGBoost Refined'] = evaluation(xgbrefined, Xtest, ytest)
model_comparison

# Draw the confusion matrix
sns.heatmap(confusion_matrix(ytest, xgbrefined.predict(Xtest)), annot=True, cmap='Reds', fmt='.4g')
plt.xlabel('Predicted label')
plt.ylabel('Training label')
plt.title('Confusion Matrix')
plt.show()

"""#***Using the trained model on the Test Data***"""

#Features on the test data
test_features = bert_feature(test['sentence'])

#Downloading the model for streamlit
joblib.dump(xgbrefined, 'XGBOOSTFINAL.pkl')

preds =xgbrefined.predict(test_features)#<--- INSERT MODEL NAME (xgbrefined)

Output = pd.DataFrame()
Tdata = pd.read_csv('https://raw.githubusercontent.com/GroupTissot/DSMLFinalProject/main/unlabelled_test_data.csv')

Output['id'] = Tdata['id']
Output['difficulty'] = pd.Series(preds).map({0:'A1', 1:'A2', 2:'B1', 3:'B2', 4:'C1', 5:'C2'})

Name = 'xgbrefined' #<--- INSERT MODEL NAME
Output.to_csv(Name, index=0)