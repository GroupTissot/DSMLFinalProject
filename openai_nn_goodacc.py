# -*- coding: utf-8 -*-
"""OpenAI_NN_goodacc

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1blBejbcdWwzka50b720hfjdfeFqTDGPD
"""

!pip install tiktoken
!pip install cohere
!pip install openai pandas numpy tensorflow scikit-learn
!pip install sentencepiece
!pip install tensorflow

!pip install --upgrade tensorflow

import openai
import pandas as pd
import numpy as np
import tensorflow as tf
import requests
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

# Function to get embeddings from OpenAI's GPT model in batches
def get_embeddings_batch(texts, model="text-embedding-ada-002", batch_size=20, api_key="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"):
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer 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"
    }
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        data = {
            "input": batch_texts,
            "model": model
        }
        response = requests.post("https://api.openai.com/v1/embeddings", json=data, headers=headers)

        if response.status_code == 200:
            batch_embeddings = [item['embedding'] for item in response.json()['data']]
            all_embeddings.extend(batch_embeddings)
        else:
            # Append None for each text in the batch if there's an error
            all_embeddings.extend([None] * len(batch_texts))

    return np.array(all_embeddings)

# Load the training and test data
training_data = pd.read_csv("https://raw.githubusercontent.com/GroupTissot/DSMLFinalProject/main/unchanged_trainingdata.csv")
test_data = pd.read_csv("https://raw.githubusercontent.com/GroupTissot/DSMLFinalProject/main/unlabelled_test_data.csv")

# Encode the difficulty levels
label_encoder = LabelEncoder()
training_data['difficulty_encoded'] = label_encoder.fit_transform(training_data['difficulty'])

# Get embeddings for the training and test data
train_embeddings = get_embeddings_batch(training_data['sentence'].tolist())
test_embeddings = get_embeddings_batch(test_data['sentence'].tolist())

# Convert embeddings to numpy arrays
X_train = np.array(train_embeddings)
y_train = np.array(training_data['difficulty_encoded'].values)
X_test = np.array(test_embeddings)

# Define the model with dropout for regularization
learning_rate = 0.001  # Custom learning rate
# Define the model with increased dropout for additional regularization
model = tf.keras.Sequential([
    Dense(188, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(144, activation='relu'),
    Dense(64, activation='relu'),
    Dense(len(label_encoder.classes_), activation='softmax')
])
model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])



# Train the model
history = model.fit(
    X_train, y_train,
    epochs=10,  # Set to a higher number, early stopping will terminate the process
    batch_size=32,
    validation_split=0.2,  # Use part of the training data for validation

)



# Predicting for test data
test_predictions = model.predict(X_test)
predicted_labels = np.argmax(test_predictions, axis=1)
test_data['predicted_difficulty'] = label_encoder.inverse_transform(predicted_labels)

# Save predictions to CSV
result_table = test_data[['id', 'predicted_difficulty']]
result_table.to_csv("predicted_difficulties.csv", index=False)
print("Predictions saved to 'predicted_difficulties.csv'")